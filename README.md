# AI_HW6
پیاده‌سازی و مقایسه جامع الگوریتم‌های یادگیری تقویتی (Q-Learning, DQN, DDPG) روی محیط MountainCar. این پروژه عملکرد این الگوریتم‌ها را در دو فضای اقدام گسسته و پیوسته ارزیابی کرده و به تحلیل چالش‌هایی مانند پایداری آموزش و پاداش پراکنده می‌پردازد.
# تحلیل جامع الگوریتم‌های یادگیری تقویتی بر روی مسئله MountainCar

## 📜 مقدمه
این پروژه یک بررسی عمیق و مقایسه‌ای بین الگوریتم‌های کلاسیک و مدرن یادگیری تقویتی است. هدف اصلی، درک عملی نقاط قوت و ضعف هر الگوریتم در مواجهه با چالش‌های رایج RL مانند فضاهای حالت پیوسته، اقدامات پیوسته، و پاداش پراکende است. محیط `MountainCar` به دلیل سادگی در عین داشتن چالش‌های ذاتی (نیاز به اکتشاف هوشمندانه)، به عنوان بستر آزمایش انتخاب شد.

---

## 🤖 الگوریتم‌ها و روش‌شناسی

### بخش ۱: محیط با اقدامات گسسته (`MountainCar-v0`)
در این بخش، فضای حالت پیوسته با یک گرید 30x30 گسسته‌سازی شد تا الگوریتم‌های جدولی قابل استفاده باشند.
* **Q-Learning**: به عنوان مدل پایه برای یادگیری تابع ارزش حالت-اقدام پیاده‌سازی شد.
* **Double Q-Learning**: برای حل مشکل تخمین خوش‌بینانه مقادیر Q در الگوریتم استاندارد، با استفاده از دو جدول مجزا پیاده‌سازی شد.
* **Deep Q-Network (DQN)**: به عنوان جایگزینی برای جدول، یک شبکه عصبی سه لایه برای تقریب تابع ارزش پیاده‌سازی شد. برای تحلیل حساسیت این الگوریتم، شش آزمایش مجزا با ترکیبی از دو بهینه‌ساز (`Adam`, `RMSprop`)، دو تابع هزینه (`MSE`, `Smooth L1 Loss`)، و دو تابع پاداش (پیش‌فرض و **شکل‌دهی شده**) انجام گرفت.

### بخش ۲: محیط با اقدامات پیوسته (`MountainCarContinuous-v0`)
در این بخش، با چالش فضای اقدام پیوسته `[-1, 1]` روبرو شدیم.
* **DQN با گسسته‌سازی اقدام**: یک رویکرد ترکیبی که در آن، فضای اقدام پیوسته به ۱۱ نقطه گسسته تبدیل شد تا بتوان از معماری DQN استفاده کرد.
* **DDPG (Deep Deterministic Policy Gradient)**: یک الگوریتم پیشرفته Actor-Critic که به طور بومی برای فضاهای اقدام پیوسته طراحی شده است. این الگوریتم با چهار شبکه مجزا (Actor, Critic و شبکه‌های هدف آنها) و مکانیزم به‌روزرسانی نرم (Soft Update) پیاده‌سازی شد.

---

## 🔬 نتایج و تحلیل‌های کلیدی

این پروژه چندین یافته مهم را به همراه داشت:
1.  **کارایی الگوریتم‌های کلاسیک**: در محیط گسسته، الگوریتم‌های ساده جدولی، به ویژه **Double Q-Learning**، با زمان آموزش کمتر، به سیاستی بهینه‌تر و با نرخ موفقیت بالا (88%) دست یافتند. این نشان می‌دهد که برای مسائل با ابعاد پایین، پیچیدگی اضافی همیشه بهتر نیست.
2.  **اهمیت حیاتی تابع پاداش**: موفقیت چشمگیر (نرخ موفقیت ۱۰۰٪) مدل DQN تنها با **مهندسی پاداش (Reward Shaping)** ممکن شد. این موضوع ثابت می‌کند که کیفیت و چگالی سیگنال پاداش، نقشی حیاتی‌تر از خود معماری الگوریتم در موفقیت عامل دارد.
3.  **چالش پاداش پراکنده**: الگوریتم **DDPG** و اکثر مدل‌های پایه **DQN** به دلیل پاداش پراکنده محیط، در یادگیری شکست خوردند. نمودارهای Loss همگرا در DDPG به خوبی نشان دادند که الگوریتم با موفقیت یک سیاست "بد" (سکون برای کمینه کردن جریمه) را یاد گرفته است.
4.  **پدیده سیاست شکننده (Brittle Policy)**: مدل `DQN (Discretized)` با وجود نمودارهای آموزش بی‌نقص، در ارزیابی کاملاً حریصانه (بدون اکتشاف) شکست خورد. این نشان داد که سیاست یادگرفته شده برای فرار از بهینه‌های محلی، به مقدار کمی نویز اکتشافی وابسته بوده است.

---

## 🚀 راهنمای نصب و اجرا

1.  **نصب پیش‌نیازها:**
    ```bash
    pip install gymnasium[classic_control] torch matplotlib numpy
    ```
2.  **اجرا:**
    * برای اجرای پروژه، نوت‌بوک `main.ipynb` (یا نام فایل اصلی شما) را در یک محیط مانند Google Colab یا Jupyter اجرا کنید.
    * استفاده از **GPU** برای آموزش مدل‌های مبتنی بر شبکه عصبی (DQN, DDPG) به شدت توصیه می‌شود.
    * تمام کدها با یک `seed` ثابت اجرا شده‌اند تا نتایج کاملاً قابل تکرار باشند. کافی است تمام سلول‌ها را به ترتیب اجرا نمایید.
